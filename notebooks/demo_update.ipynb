{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d32c37f-4d35-40f1-a319-f21025a2dfbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import faiss\n",
    "import nltk\n",
    "\n",
    "from openai import OpenAI\n",
    "from PyPDF2 import PdfReader\n",
    "from dotenv import load_dotenv\n",
    "from textwrap import dedent\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from textwrap import dedent\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ca35d8-760d-47d8-9e7a-c6d67c40c3f0",
   "metadata": {},
   "source": [
    "# Medical QnA Bot\n",
    "\n",
    "Designing a bot that takes in a user query and only considers our own data source to ask relevant follow-up questions and guides the user towards relevant sections of the data source.\n",
    "\n",
    "![title](system.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2637fd0f-2b3a-4d77-a167-80cc5466ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = []\n",
    "    for page in reader.pages:\n",
    "        text.append(page.extract_text())\n",
    "    return \"\\n\".join(text)\n",
    "\n",
    "def get_stop_words():\n",
    "    stop_words = list(stopwords.words('english'))\n",
    "    stop_words.remove(\"no\")\n",
    "    return stop_words\n",
    "\n",
    "def process_text(text, stop_words):\n",
    "    text = re.sub('[^A-Za-z0-9]+', ' ', text).strip()\n",
    "    tokens = [w for w in text.split() if w not in stop_words] # word_tokenize(text.lower())\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def generate_embeddings(text, model):\n",
    "    embedding = client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "    return embedding\n",
    "\n",
    "def save_embedding_chunks(pdf_text_processed, model, embed_file, avg_token_len=4, max_tokens=1024, overlap=512):\n",
    "    chunk_len = avg_token_len * max_tokens\n",
    "    text_list = []\n",
    "    embed_list = []\n",
    "    for i in tqdm(range(0, len(pdf_text_processed)-chunk_len+overlap, chunk_len-overlap)):\n",
    "        text = pdf_text_processed[i:i+chunk_len]\n",
    "        embed = generate_embeddings(text, model)\n",
    "        text_list.append(text)\n",
    "        embed_list.append(embed)\n",
    "    \n",
    "    df = pd.DataFrame({'text': text_list, 'embedding': embed_list})\n",
    "    df.to_csv(embed_file, index=False)\n",
    "    print(f\"Saved Vector Embeddings at: {embed_file}\")\n",
    "\n",
    "def cosine(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def compute_similarity(embeddings, query_embed, top_k=5):\n",
    "    embed_scores = embeddings.apply(\n",
    "        lambda x: (x['text'], x['embedding'], cosine(x['embedding'], query_embed)), axis=1\n",
    "    )\n",
    "    embed_scores_df = pd.DataFrame(embed_scores.tolist(), columns=['text', 'embedding', 'score'])\n",
    "    sim_df = embed_scores_df.sort_values(by='score', ascending=False)[:top_k]\n",
    "    return sim_df\n",
    "\n",
    "def valid_response(embedding_df, query_embed, threshold=0.6, top_k=5):\n",
    "    sim_df = compute_similarity(embedding_df, query_embed, top_k=top_k)\n",
    "    scores = sim_df['score'].values\n",
    "    if np.any(scores > threshold):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def duplicate_response(previous_responses, resp_embed, threshold=0.9):\n",
    "    #return any(np.allclose(resp_embed, prev_resp, atol=1e-2) for prev_resp in previous_responses) \n",
    "    for pr_embed in previous_responses:\n",
    "        cos = round(cosine(pr_embed, resp_embed), 2)\n",
    "        if cos >= threshold:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_satisfied(query_response, user_input):\n",
    "    if \"satisfied with response\" in query_response.lower() and \"yes\" in user_input.lower():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "##### Vectorized implementation of MMR (maximal marginal relevance) #####\n",
    "    \n",
    "def diversity_ranking(query_embed, selected_docs, unselected_docs, lambda_=0.7, top_k=5):\n",
    "    # convert embeddings to np arrays\n",
    "    docs = [d[0] for d in unselected_docs]\n",
    "    usd = np.array([d[1] for d in unselected_docs])\n",
    "    sd = np.array([d[1] for d in selected_docs])\n",
    "    q = np.array(query_embed).reshape(1, -1)\n",
    "    \n",
    "    # similarity of query with all unselected docs\n",
    "    cos_one = np.dot(q, usd.T) / (np.linalg.norm(q, axis=1).reshape(-1, 1) * np.linalg.norm(usd, axis=1).reshape(1, -1))\n",
    "    \n",
    "    # similarity of selected docs with all unselected docs\n",
    "    cos_two = np.dot(sd, usd.T) / (np.linalg.norm(sd, axis=1).reshape(-1, 1) * np.linalg.norm(usd, axis=1).reshape(1, -1))\n",
    "    cos_two = np.max(cos_two, axis=0).reshape(1, -1)\n",
    "    \n",
    "    # compute mmr scores and create df\n",
    "    score = cos_one * lambda_ - (1 - lambda_) * cos_two\n",
    "    score = score.flatten().tolist()\n",
    "\n",
    "    # return top_k docs with highest MMR (maximal marginal relevance)\n",
    "    score_df = pd.DataFrame({'doc': docs, 'score': score})\n",
    "    \n",
    "    top_k_idx = score_df['score'].nlargest(top_k).index\n",
    "    return score_df.iloc[top_k_idx]\n",
    "\n",
    "\n",
    "def create_faiss_index(embedding_df, faiss_out, embed_col='embedding'):\n",
    "    embedding_df[embed_col] = embedding_df[embed_col].apply(lambda x: json.loads(x))\n",
    "    embeddings = np.vstack(embedding_df[embed_col].values.tolist())\n",
    "    d = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, faiss_out)\n",
    "    print(f'Saving Faiss Index Locally at: {faiss_out}')\n",
    "    return index\n",
    "\n",
    "def faiss_top_k(df, query_embed, index, top_k=5):\n",
    "    search_embed = np.array(query_embed).reshape(1, -1)\n",
    "    S, I = index.search(search_embed, k=top_k)\n",
    "    docs = df.iloc[I[0].tolist()].values.tolist()\n",
    "    return docs, S, I\n",
    "    \n",
    "#selected_docs, S, I = faiss_top_k(embedding_df, query_embed, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d95c5d6-b3c1-4f94-b77c-b25a376835fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize openai client\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57b27984-5b53-4c80-ad98-5f4b5fffc001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "858137"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pdf file as text/str\n",
    "file_path = '/Users/abdulrafeytahir/Desktop/healthcare-demo-project/notebooks/guideline-for-the-diagnosis-and-management-of-aortic-disease-a-report-of-the.pdf'\n",
    "pdf_text = read_pdf(file_path)\n",
    "len(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3d2ec95-ae1b-49a4-9983-503b18ba14fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704659"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process pdf_text by removing special characters and stop words\n",
    "stop_words = get_stop_words()\n",
    "pdf_text_processed = process_text(pdf_text, stop_words)\n",
    "len(pdf_text_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb853076-961b-4e3e-bf16-ed49f2e1d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate embeddings\n",
    "# model = \"text-embedding-3-small\"\n",
    "# embed_file = \"overlap_embeddings_1k.csv\"\n",
    "# save_embedding_chunks(pdf_text_processed, model, embed_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc1f07b5-8009-4f25-a0a7-a9ce8ed0699e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding file\n",
    "embed_file = \"overlap_embeddings_1k.csv\"\n",
    "embedding_df = pd.read_csv(embed_file)\n",
    "embedding_df['embedding'] = embedding_df['embedding'].apply(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2063e1f-4ffe-4134-8529-d03df1746faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a faiss index locally\n",
    "# index_path = \"overlap_embeddings.faiss\"\n",
    "# embeddings = pd.read_csv(embed_file)\n",
    "# index = create_faiss_index(embeddings, faiss_out=index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd5fac-1b97-44c6-8dba-0416186f1e8c",
   "metadata": {},
   "source": [
    "### QnA Bot\n",
    "\n",
    "The Embedding model being used is `text-embedding-3-small` from OpenAI. The text generation model is `GPT-3.5-Turbo`\n",
    "\n",
    "The bot maintains all previous interactions in the query string. It also adds context by using all previous\n",
    "queries and fetching `top_k` documents from the vector store and makes conversation based on that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e62a40be-eefc-422d-b8e6-13c555fa27db",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"text-embedding-3-small\"\n",
    "chat_model = \"gpt-3.5-turbo\"\n",
    "query = \"How should I treat my 50 year old patient with aortic aneurysm\"\n",
    "query = process_text(query, stop_words)\n",
    "queries = [query]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f916ce-f85d-417f-8d62-03dbecc6e25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Is the patient showing any symptoms or complications related to the thoracic aortic aneurysm (TAA)?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " yes \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What specific symptoms or complications related to the thoracic aortic aneurysm (TAA) is the 50-year-old patient experiencing?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " they have chest pain and shortness of breath\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Is the patient currently receiving any treatment for their chest pain and shortness of breath related to the thoracic aortic aneurysm (TAA)?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " not at the moment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the symptoms of chest pain and shortness of breath related to the thoracic aortic aneurysm (TAA) in the 50-year-old patient, it is important to consider immediate medical intervention. Here are some follow-up questions to gather more information:\n",
      "\n",
      "1. Has the patient been diagnosed with a thoracic aortic aneurysm (TAA) before experiencing these symptoms?\n",
      "2. Are there any other medical conditions or risk factors that the patient has that could be contributing to the symptoms?\n",
      "3. Has the patient had any previous imaging or diagnostic tests done for the thoracic aortic aneurysm (TAA)?\n",
      "4. Is the patient currently on any medications that could be affecting their symptoms?\n",
      "\n",
      "Please provide this information so that we can determine the appropriate course of action for the patient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " the patient also has diabetes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given the patient's symptoms of chest pain and shortness of breath, along with a known diagnosis of thoracic aortic aneurysm (TAA) and diabetes, it is crucial to address these concerns promptly. Here are some recommendations based on the information provided:\n",
      "\n",
      "1. **Immediate Medical Attention**: Due to the symptoms of chest pain and shortness of breath, it is advisable for the patient to seek urgent medical evaluation at a healthcare facility.\n",
      "\n",
      "2. **Management of Thoracic Aortic Aneurysm (TAA)**: The patient's TAA needs to be assessed for size and potential risk of complications. Treatment options may include medical therapy, endovascular stenting, or open surgical repair, depending on the size and location of the aneurysm.\n",
      "\n",
      "3. **Diabetes Management**: Given the coexistence of diabetes, it is important to ensure that blood sugar levels are well-controlled to prevent any complications that could exacerbate the TAA symptoms.\n",
      "\n",
      "4. **Cardiovascular Evaluation**: The patient may require further cardiovascular evaluation, including imaging studies such as CT or MRI to assess the extent of the TAA and its impact on surrounding structures.\n",
      "\n",
      "5. **Medication Review**: It is essential to review the patient's current medications to ensure they are not exacerbating the symptoms or interacting with any new treatments that may be prescribed.\n",
      "\n",
      "Would you like more information on the treatment options for thoracic aortic aneurysm (TAA) or any other specific details regarding the patient's condition?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What specific information would you like to know more about regarding the treatment options for thoracic aortic aneurysm (TAA) in the 50-year-old patient with chest pain, shortness of breath, and diabetes?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " what medication can be given to this patient\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Given the patient's thoracic aortic aneurysm (TAA) along with chest pain, shortness of breath, and diabetes, the management may involve the following medications:\n",
      "\n",
      "1. **Blood Pressure Control**: Medications to control blood pressure are crucial in managing aortic aneurysms. This may include beta-blockers, calcium channel blockers, or ACE inhibitors to reduce the risk of aortic dissection.\n",
      "\n",
      "2. **Diabetes Management**: For diabetes control, medications such as metformin, insulin, or other oral hypoglycemic agents may be prescribed to maintain blood sugar levels within the target range.\n",
      "\n",
      "3. **Pain Management**: Analgesics may be given to manage chest pain associated with the thoracic aortic aneurysm.\n",
      "\n",
      "4. **Antiplatelet Therapy**: Depending on the patient's overall cardiovascular risk, antiplatelet medications like aspirin may be considered to reduce the risk of clot formation.\n",
      "\n",
      "5. **Cholesterol Management**: Statins may be prescribed to manage cholesterol levels and reduce the risk of atherosclerosis, which can contribute to aortic aneurysm progression.\n",
      "\n",
      "It is important to note that the specific medications and dosages will be determined by the patient's healthcare provider based on their individual medical history, current condition, and any contraindications.\n",
      "\n",
      "Would you like more information on the specific medications used in the management of thoracic aortic aneurysm (TAA) or any other details related to the patient's treatment plan?, yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " no\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What specific information would you like to know more about regarding the medications used in the management of thoracic aortic aneurysm (TAA) in the 50-year-old patient with chest pain, shortness of breath, and diabetes?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " thanks for the information, i don't need anything else\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You're welcome! If you have any more questions in the future or need further assistance, feel free to reach out. Take care!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " okay thank\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I don't know the answer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ok\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "# Initialize user interaction and setup\n",
    "user_satisfied = False\n",
    "previous_responses = []\n",
    "messages = [{}]\n",
    "\n",
    "while not user_satisfied:\n",
    "    # Generate query embeddings and perform retrieval and ranking\n",
    "    query_embed = generate_embeddings(query, embedding_model) #\n",
    "    selected_docs = compute_similarity(embedding_df, query_embed, top_k=5)\n",
    "    unselected_docs = embedding_df.loc[~embedding_df.index.isin(selected_docs.index)]\n",
    "\n",
    "    # Document Ranking using Maximum Marginal Relevance to select top_k docs (using higher lambda for relevance)\n",
    "    # TODO: figure out a mechanism to tweak values of lambda in cases of invalid or duplicate responses.\n",
    "    ranked_docs = diversity_ranking(query_embed, selected_docs.values.tolist(), unselected_docs.values.tolist(), lambda_=0.7, top_k=5)\n",
    "    context = '\\n'.join([d[0] for d in ranked_docs.values.tolist()])\n",
    "\n",
    "    # System message setup\n",
    "    system_msg = dedent(f\"\"\"\n",
    "        You are a medical expert. You can only use the information provided in the context that comes from \n",
    "        our own data source to response. When asked a question, you will only ask relevant follow-up questions\n",
    "        till you are able to find relevant sections of the data/information required by the user. If you cannot \n",
    "        answer, then respond with I don't know the answer. Once relevant information is provided to the user,\n",
    "        ask them if they are satisfied with response (yes/no), if they answer is yes, then terminate the conversation. \n",
    "        Also, make sure NOT to ask duplicate questions based on previous chat history.\n",
    "        Context: {context}\n",
    "        \"\"\")\n",
    "\n",
    "    messages[0] = {\"role\": \"system\", \"content\": system_msg}\n",
    "    messages.append({\"role\": \"user\", \"content\": query})\n",
    "    \n",
    "    \n",
    "    # Get model response\n",
    "    response = client.chat.completions.create(model=chat_model, messages=messages, temperature=0)\n",
    "    query_response = response.choices[0].message.content\n",
    "\n",
    "    # Process and generate embedding for the response for duplication check\n",
    "    resp_processed = process_text(query_response, stop_words)\n",
    "    resp_embed = generate_embeddings(resp_processed, embedding_model)\n",
    "\n",
    "    # Check for duplicate response\n",
    "    if duplicate_response(previous_responses, resp_embed):\n",
    "        msg = \"It is a duplicate response, generate a new response.\"\n",
    "        messages.append({\"role\": \"user\", \"content\": msg})\n",
    "\n",
    "    # Display and verify the response\n",
    "    print(f\"\\n{query_response}\")\n",
    "    previous_responses.append(resp_embed)  # Store the response embedding to check for future duplicates\n",
    "\n",
    "    user_input = input() \n",
    "    user_satisfied = is_satisfied(query_response, user_input)\n",
    "        \n",
    "    query = dedent(f\"{query_response}, {user_input}\")\n",
    "    queries.append(process_text(query, stop_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f1dd98-a567-4a02-ad99-f61a59684376",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
